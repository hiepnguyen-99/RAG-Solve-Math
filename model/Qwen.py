import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline

# 1. Thi·∫øt b·ªã
device = "cuda" if torch.cuda.is_available() else "cpu"
print("ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã:", device)

# 2. M√¥ h√¨nh embedding
embedding_model = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-base",
    model_kwargs={"device": device}
)

# 3. Load Chroma DB
chroma_path = "./chroma_db"
db = Chroma(persist_directory=chroma_path, embedding_function=embedding_model, collection_name="math_vectors"  # th√™m d√≤ng n√†y!
)
retriever = db.as_retriever(search_kwargs={"k": 3})
print("S·ªë l∆∞·ª£ng t√†i li·ªáu trong Chroma:", db._collection.count())
print(f"Loading Chroma from: {chroma_path}")

# 4. T·∫£i m√¥ h√¨nh ng√¥n ng·ªØ (LLM)
model_name = "Qwen/Qwen2-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.float16
).to(device)

# 5. T·∫°o pipeline sinh vƒÉn b·∫£n
generation_pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.7,
    top_p=0.9,
    do_sample=False,
    device=0 if device == "cuda" else -1,
    framework="pt"
)
llm = HuggingFacePipeline(pipeline=generation_pipe)

# 6. Prompt template c·∫≠p nh·∫≠t
prompt_template = PromptTemplate.from_template(
"""B·∫°n l√† tr·ª£ l√Ω To√°n h·ªçc. H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c nh·∫•t t·ª´ c√°c th√¥ng tin sau."
Th√¥ng tin truy h·ªìi:
---------------------
{context}
---------------------
Tr·∫£ l·ªùi:"""
)

# 7. T·∫°o Retrieval-QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True
)

# 8. V√≤ng l·∫∑p h·ªèi-ƒë√°p
print("=== H·ªá th·ªëng truy v·∫•n To√°n h·ªçc ===")
print("Nh·∫≠p 'exit' ƒë·ªÉ tho√°t.")
print("----------------------------------------")

while True:
    question = input("C√¢u h·ªèi c·ªßa b·∫°n: ").strip()
    if question.lower() == "exit":
        break

    result = qa_chain.invoke({"query": question})
    answer = result["result"].strip()

    # In c√°c t√†i li·ªáu ƒë√£ truy h·ªìi
    if result["source_documents"]:
        print("\nüìö Truy h·ªìi c√°c t√†i li·ªáu:")
        for doc in result["source_documents"]:
            print(f"\n---\n{doc.page_content}\n---")
    else:
        print("\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan trong Chroma.")

    # N·∫øu kh√¥ng c√≥ tr·∫£ l·ªùi ho·∫∑c kh√¥ng c√≥ t√†i li·ªáu ‚Üí fallback
    if not result["source_documents"] or answer == "":
        fallback_prompt = f"C√¢u h·ªèi: {question}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn:"
        answer = llm.invoke(fallback_prompt).strip()

    print("\n‚û°Ô∏è C√¢u tr·∫£ l·ªùi:", answer)
    print("----------------------------------------")
